# 🧠 Conversational AI Chat App (FastAPI + Gradio + Gemini)

This is a production-ready Conversational AI application built as part of a 24-hour challenge for Yuñ. It uses **FastAPI** for the backend MCP server, **Gradio** for a clean chat frontend, and supports LLM integration including **Google Gemini**, **OpenAI**, and open-source models like **Mistral**.

---

## ⚙️ Features

- ✅ Conversational AI via Google Gemini (default)
- 🔄 Switch between LLMs like OpenAI, Claude, and Mistral
- 💬 Context-aware chat via FastAPI and Gradio
- 🐳 Dockerized backend & frontend
- 🚀 Runs locally and deploys easily

---

## 🧱 Tech Stack

- [FastAPI](https://fastapi.tiangolo.com/) – Backend server
- [Gradio](https://www.gradio.app/) – Frontend UI
- [Google Generative AI](https://ai.google.dev/) – Gemini model
- [Docker & Docker Compose](https://docs.docker.com/) – Containerization
- Python 3.11

---

## 🚀 Run Locally with Docker

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/conversational-ai-app.git
cd conversational-ai-app
