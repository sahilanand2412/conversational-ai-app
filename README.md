# ğŸ§  Conversational AI Chat App (FastAPI + Gradio + Gemini)

This is a production-ready Conversational AI application built as part of a 24-hour challenge for YuÃ±. It uses **FastAPI** for the backend MCP server, **Gradio** for a clean chat frontend, and supports LLM integration including **Google Gemini**, **OpenAI**, and open-source models like **Mistral**.

---

## âš™ï¸ Features

- âœ… Conversational AI via Google Gemini (default)
- ğŸ”„ Switch between LLMs like OpenAI, Claude, and Mistral
- ğŸ’¬ Context-aware chat via FastAPI and Gradio
- ğŸ³ Dockerized backend & frontend
- ğŸš€ Runs locally and deploys easily

---

## ğŸ§± Tech Stack

- [FastAPI](https://fastapi.tiangolo.com/) â€“ Backend server
- [Gradio](https://www.gradio.app/) â€“ Frontend UI
- [Google Generative AI](https://ai.google.dev/) â€“ Gemini model
- [Docker & Docker Compose](https://docs.docker.com/) â€“ Containerization
- Python 3.11

---

## ğŸš€ Run Locally with Docker

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/conversational-ai-app.git
cd conversational-ai-app
